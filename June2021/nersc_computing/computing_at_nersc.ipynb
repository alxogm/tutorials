{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e429a-333d-4adf-826f-6db6c9b8b205",
   "metadata": {},
   "source": [
    "# Cluster Computing with NERSC - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82460833-db85-4232-bfa1-0009413f184f",
   "metadata": {},
   "source": [
    "The goal of this notebook is to introduce NERSC and help everyone to learn to work in it. This means that you will get familiar with NERSC's module and queues' systems, storage policies, setting up an enviroment, work with Jupyter and set a job to the different queues.\n",
    "\n",
    "This notebook will be brief, but you can look at NERSC's documentation [here](https://docs.nersc.gov)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b52551-d74c-4878-a110-4d879f952643",
   "metadata": {},
   "source": [
    "## Storage systems\n",
    "\n",
    "NERSC works with different file systems for multiple purposes. Usually, you will be working in either your ```$HOME``` or ```$SCRATCH``` directories. Further information about the storage systems is found [here](https://docs.nersc.gov/filesystems/).\n",
    "\n",
    "- **HOME**: is a permanent storage system with small capacity (40 GB) recommended for code, scripts and stuff you may want to keep. \n",
    "\n",
    "- **SCRACTH** is a large capacity system (20 TB) intended for temporary storage since it has a 12 week purge limit for files that have not been read!.\n",
    "\n",
    "For the moment lets work in SCRATCH.\n",
    "\n",
    "```\n",
    "$ cd $SCRATCH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60263bc1-89f2-4c26-8c1c-2ae44ea3a222",
   "metadata": {},
   "source": [
    "## Modules\n",
    "Working on a cluster usually means working with modular systems, NERSC is no exception. The main advantage of this is that you can manage multiple softwares without breaking something (or not so easily!) and use multiple versions of these softwares too. \n",
    "\n",
    "Let's see which modules you can use.\n",
    "\n",
    "By default you have some modules loaded when logging into your accout, you can check this by typing in your terminal:\n",
    "\n",
    "```\n",
    "$ module list\n",
    "```\n",
    " \n",
    "But, which other modules can you use in NERSC? There are lots of them! \n",
    "\n",
    "You can check all the available modules with\n",
    "\n",
    "```\n",
    "$ module avail\n",
    "```\n",
    "\n",
    "How do I use a specific module? And what does it do? \n",
    "The latter can be checked with ```module show``` command: e.g\n",
    "\n",
    "```\n",
    "$ module show python\n",
    "```\n",
    "\n",
    "The former can be done with ```module load```.\n",
    "\n",
    "\n",
    "Lets load the python module:\n",
    "\n",
    "```\n",
    "$ module load python\n",
    "```\n",
    "\n",
    "You can check with ```module list``` that you now have an additional module.\n",
    "\n",
    "You can find further information about modules at NERSC [here](https://docs.nersc.gov/environment/modules/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf5d61-1194-423d-9b88-e54566ffba5f",
   "metadata": {},
   "source": [
    "## Setting up a python enviroment\n",
    "\n",
    "Now that we have python's module loaded let's create an enviroment. \n",
    "\n",
    "\n",
    "This is recommended since some codes have conflicts with some specific library versions. To avoid this you can create an enviroment for a specific task you may have. For further information about enviroment managing visit [this](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) page.\n",
    "\n",
    "Let's say that you need to use Python 3.7 and numpy 1.19.0 specifically and any astropy version for a task. This can be done with \n",
    "\n",
    "```\n",
    "$ conda create -n myenv python=3.7 numpy=1.19.0 astropy\n",
    "```\n",
    "\n",
    "Before activating your enviroment type ```pip list```, just to see that your enviroment actually has different libraries than the default NERSC's python enviroment.\n",
    "\n",
    "Now do the following:\n",
    "\n",
    "```\n",
    "$ source activate myenv\n",
    "$ pip list\n",
    "```\n",
    "\n",
    "Now you are working in your specific enviroment! You should notice this by looking at your enviroment's name at the left of your terminal e.g:  ```(myenv)$```.\n",
    "\n",
    "You can exit this enviroment by typing ```conda deactivate```.\n",
    "\n",
    "**NOTE**: Every time you want to work in your enviroment you should do the following:\n",
    "\n",
    "```\n",
    "$ module load python\n",
    "$ source activate myenv\n",
    "```\n",
    "\n",
    "Sometimes your enviroment requires multiple instructions and might be difficult to remember every step to set it up. You can avoid this struggle by creating a shell script and source it every time you want to work in this specific enviroment. For example, lets use the shell file aditional to this notebook:\n",
    "\n",
    "```\n",
    "$ source <path/to/script>/load_myenv.sh\n",
    "```\n",
    "\n",
    "This shell file can contain whatever you want! Export paths, definitions, multiple module loading, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee203111-94a0-4d71-a847-ba9ffa9222b2",
   "metadata": {},
   "source": [
    "## Working in Jupyter at NERSC\n",
    "\n",
    "So, what if I want to work on a notebook remotely? Fortunatelly we have Jupyter at NERSC! Just go to https://jupyter.nersc.gov/hub/home and log into your account.\n",
    "\n",
    "But what about a specific or custom kernel? This is a bit tricky so let's work on that!\n",
    "\n",
    "First, create a shell file with the specifications of your enviroment. An example is added to this notebook ```myenv.sh```. This file should be an executable. To do this simply run:\n",
    "```\n",
    "$ chmod u+x myenv.sh\n",
    "```\n",
    "\n",
    "Then, create a folder for your enviroment in the kernels directory of jupyter:\n",
    "\n",
    "```\n",
    "$ cd $HOME/.local/share/jupyter/kernels/\n",
    "$ mkdir myenv\n",
    "$ cd myenv\n",
    "```\n",
    "\n",
    "Finally, create a ```.json``` file for your kernel. This is usually done with ```vi kernel.sh``` and writing (or copying) the whole thing. But you can just copy the file attached to this notebook. **IMPORTANT:** in this file the second line should point to the path of the file (```myenv.sh```) you created.\n",
    "\n",
    "**NOTE:** This is done just once per kernel.\n",
    "\n",
    "Let's see that we are using our enviroment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d211b8-bcd4-47d8-ad94-2c3b2b4f3f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aebfd-2e58-4325-9c40-db06b38daf15",
   "metadata": {},
   "source": [
    "It works!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e037e-7ffe-428c-996d-0708c8401e70",
   "metadata": {},
   "source": [
    "## Using DESICODE modules\n",
    "\n",
    "If you want to use desicode, in a **FRESH NEW** terminal type the following:\n",
    "\n",
    "```\n",
    "$ source /project/projectdirs/desi/software/desi_environment.sh master\n",
    "```\n",
    "\n",
    "The last part of this line can vary depending on the version of the desicode you want to use, ```master``` uses the libraries as they are up to date, but you can load stable versions, the latest is ```21.5```.\n",
    "\n",
    "To create a Jupyter kernel simply use the folllowing:\n",
    "\n",
    "```\n",
    "$ source /project/projectdirs/desi/software/activate_desi_jupyter.sh master\n",
    "```\n",
    "Once again you can substitute master with any stable version.\n",
    "\n",
    "\n",
    "**NOTE:** activating the jupyter enviroment for DESI should be done only ONCE per version you want to install. I recommend master and any stable version you like.\n",
    "\n",
    "You may want to create a custom DESI enviroment, however this is a quite specific task, so we refer to the [wiki page](https://desi.lbl.gov/trac/wiki/Pipeline/GettingStarted/NERSC#CustomizingYourDESIPythonEnvironment) that helps you to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13113e2-afe3-4e7a-bfea-9fa4f77a10e5",
   "metadata": {},
   "source": [
    "# Submitting Jobs\n",
    "\n",
    "Now that we have set up our working enviroments (or where you will most likely be working) is time to send a job to NERSC.\n",
    "\n",
    "## Queue policy\n",
    "\n",
    "NERSC has different queues with some restrictions according with the requirements of the user. The usual queues you will be working (unless some special requirement) are:\n",
    "\n",
    "- **Regular**: the standard queue for jobs, has a limit in computing time of 48 hours and a maximum of 5000 submitted jobs without run limit. There is no limit in the number of requested nodes, meaning that you can use up to 1932 for Haswell and 9489 for KNL. Click [here](https://docs.nersc.gov/systems/cori/) for more information about Haswell and KNL.\n",
    "\n",
    "- **Interactive**: Usually used for code development, test, debug and analysis. Has a time limit of 4 hours, can run up to 2 simultaneous jobs and this is also the limit of submitted jobs. There is a maximum requested node number of 64. Jobs in interactive should be run via salloc or srun (we will get to this). \n",
    "\n",
    "- **Debug**: Used for code testing and debugging, or jobs that don't require much time to run. Has a time limit of 30 minutes, can run up to 2 simultaneous jobs, with a limit of 5 submitted jobs. You can request up to 64 nodes.\n",
    "\n",
    "For other queues you can check [here](https://docs.nersc.gov/jobs/policy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246396c0-c149-4e89-986d-82c5b3b00eb3",
   "metadata": {},
   "source": [
    "## Submitting jobs with salloc or srun\n",
    "\n",
    "You can run jobs directly from the command line with ```salloc``` or ```srun```. \n",
    "\n",
    "The command ```salloc``` is used to allocate resources and work interactively, this is commmonly used with the interactive queue.\n",
    "\n",
    "While ```srun``` is used to submit a single job, this instruction can be used inside any batch script, during an allocation in interactive queue or directly from the command line.\n",
    "\n",
    "Aditionally to these commands you should use options (flags) for the job, this includes the number of nodes, the queue, time and computing system to be used, however there are multiple options that can be set up, these can be found [here](https://docs.nersc.gov/jobs/#commonly-used-options).\n",
    "\n",
    "An example of an salloc command is the following:\n",
    "\n",
    "```\n",
    "$ salloc -A desi -C haswell -q interactive -t 00:10:00 -N 1\n",
    "```\n",
    "\n",
    "In this command we are requesting 10 minutes of one node of haswell in the interactive queue (you can change haswell for knl), ask up to 64 nodes and up to 4 hours. This will prompt a interactive node in your terminal. You should notice it when your terminal has a ```@nid``` number next to your user. Once you are granted job allocation (Nodes are ready for job) you can run a job inside this interactive session with ```srun```.\n",
    "\n",
    "\n",
    "```\n",
    "$ srun python test_script.py\n",
    "```\n",
    "\n",
    "A thing you should know is that ```srun``` inherits the properties from salloc and you can add more of these. For example, if you ask for 3 nodes with salloc ```-N 3``` you can distribute different jobs among them by writing ```srun -N 1 ```. \n",
    "\n",
    "Also ```salloc``` uses the enviroment you are currently working on in your terminal when calling it. But you can also load modules or enviroments inside the allocated node.\n",
    "\n",
    "If your job finishes before your allocation time runs out just type ```exit```.\n",
    "\n",
    "Equivalently we could have run ```srun``` directly from our command line. \n",
    "\n",
    "\n",
    "```\n",
    "$ srun -A desi -C haswell -q interactive -t 00:10:00 -N 1 python test_script.py\n",
    "```\n",
    "\n",
    "You can change the interactive queue to another kind of queue according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4235734-a869-410b-9fc2-b1be868593b8",
   "metadata": {},
   "source": [
    "## Submitting jobs with SBATCH\n",
    "\n",
    "The last piece of our tutorial is to submit jobs via sbatch. This is done via a bash script like the one attached to this notebook ```example_batch.batch```. It must include the options you want with a ```#SBATCH``` flag. Followed by the instructions you want to run. This file can contain anything after the SBATCH flags, like loading modules, calculations via another script, srun commands, for iterations, etc.\n",
    "\n",
    "To run it simply type:\n",
    "\n",
    "```\n",
    "$ sbatch example_batch.batch\n",
    "```\n",
    "\n",
    "You can check your currently running or queued jobs with \n",
    "\n",
    "```\n",
    "$ squeue -u <your_user>\n",
    "```\n",
    "\n",
    "Also, you can cancel a job with\n",
    "\n",
    "```\n",
    "$ scancel <jobid>\n",
    "``` \n",
    "or all your jobs with\n",
    "\n",
    "```\n",
    "$ scancel -u <your_user>\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ac35d-3da2-4c98-b6c1-17004d1e7f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
